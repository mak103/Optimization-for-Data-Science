{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Short Lab 3 : Proximal/cyclic/greedy coordinate descent\n",
    "\n",
    "#### Authors: M. Massias, P. Ablin\n",
    "\n",
    "## Aim\n",
    "\n",
    "The aim of this material is to code \n",
    "- cyclic and greedy coordinate descent for ordinary least squares (OLS)\n",
    "- proximal gradient descent for sparse Logistic regression\n",
    "\n",
    "## VERY IMPORTANT\n",
    "\n",
    "- This work **must be done by pairs of students**.\n",
    "- **Each** student must send their work **before the 11th of november at 23:59**, using the **moodle platform**.\n",
    "- This means that **each student in the pair sends the same file**\n",
    "- On the moodle, in the \"Optimization for Data Science\" course, you have a \"devoir\" section called **Rendu TP du 5 novembre 2017**. This is where you submit your jupyter notebook file. \n",
    "- The **name of the file must be** constructed as in the next cell\n",
    "\n",
    "# Gentle reminder: no evaluation if you don't respect this EXACTLY\n",
    "\n",
    "### How to construct the name of your file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lab3_wang_yuqing_and_shi_zhengkang.ipynb\n"
     ]
    }
   ],
   "source": [
    "# Change here using YOUR first and last names\n",
    "fn1 = \"Yuqing\"\n",
    "ln1 = \"WANG\"\n",
    "fn2 = \"Zhengkang\"\n",
    "ln2 = \"SHI\"\n",
    "\n",
    "filename = \"_\".join(map(lambda s: s.strip().lower(), \n",
    "                        [\"lab3\", ln1, fn1, \"and\", ln2, fn2])) + \".ipynb\"\n",
    "print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the usual functions:\n",
    "\n",
    "from numpy.random import multivariate_normal\n",
    "from scipy.linalg.special_matrices import toeplitz\n",
    "from numpy.random import randn\n",
    "\n",
    "\n",
    "def simu(coefs, n_samples=1000, corr=0.5, for_logreg=False):\n",
    "    n_features = len(coefs)\n",
    "    cov = toeplitz(corr ** np.arange(0, n_features))\n",
    "    A = multivariate_normal(np.zeros(n_features), cov, size=n_samples)\n",
    "    b = A.dot(coefs) + randn(n_samples)\n",
    "    if for_logreg:\n",
    "        b = np.sign(b)\n",
    "    return A, b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Ordinary Least Squares\n",
    "\n",
    "\n",
    "Let $A \\in \\mathbb{R}^{n \\times p}$, $y \\in \\mathbb{R}^n$.\n",
    "We want to use coordinate descent to solve:\n",
    "    $$\\hat w \\in  \\mathrm{arg \\, min \\,} \\frac 12 \\Vert Aw - b \\Vert ^2 $$\n",
    "\n",
    "We ask you to code:\n",
    "- cyclic coordinate descent: at iteration $t$, update feature $j = t \\mod p$\n",
    "- greedy coordinate descent: at iteration $t$, update feature having the largest partial gradient in magnitude, ie $j = \\mathrm{arg\\, max \\,}_{i} \\vert \\nabla_i f(w_t) \\vert$.\n",
    "\n",
    "\n",
    "**WARNING**: You must do this in a clever way, ie such that $p$ updates cost the same as one update of GD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_features = 100\n",
    "np.random.seed(1970)\n",
    "coefs = np.random.randn(n_features)\n",
    "\n",
    "A, b = simu(coefs, n_samples=1000, for_logreg=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cyclic_cd(A, b, n_iter):\n",
    "    n_samples, n_features = A.shape\n",
    "    all_objs = []\n",
    "    \n",
    "    w = np.zeros(n_features)\n",
    "    residuals = b - A.dot(w)\n",
    "    \n",
    "    # TODO\n",
    "    lips_const = norm(A, axis=0) ** 2\n",
    "    # END TODO\n",
    "    \n",
    "    for t in range(n_iter):\n",
    "        j = t % n_features\n",
    "        # TODO\n",
    "        old_w_j = w[j]\n",
    "        w[j] -=  (A.T.dot(A.dot(w) - b))[j]/lips_const[j]\n",
    "        # update residuals:\n",
    "        #residuals = b - A.dot(w)\n",
    "        change_w = np.zeros(n_features)\n",
    "        change_w[j] = old_w_j - w[j]\n",
    "        residuals += A.dot(change_w)\n",
    "        # END TODO\n",
    "        \n",
    "        if t % n_features == 0:\n",
    "            all_objs.append((residuals ** 2).sum() / 2.)\n",
    "    return w, np.array(all_objs)\n",
    "\n",
    "\n",
    "\n",
    "def greedy_cd(A, b, n_iter):\n",
    "    n_samples, n_features = A.shape\n",
    "    all_objs = []\n",
    "    \n",
    "    w = np.zeros(n_features)\n",
    "    \n",
    "    gradient = A.T.dot(A.dot(w) - b)\n",
    "    gram = A.T.dot(A)  # you will need this to keep the gradient up to date\n",
    "    \n",
    "    # TODO\n",
    "    lips_const = norm(A, axis=0) ** 2\n",
    "    # END TODO \n",
    "    \n",
    "    for t in range(n_iter):\n",
    "        # TODO\n",
    "        # choose feature j to update: \n",
    "        j = abs(gradient).tolist().index(max(abs(gradient)))\n",
    "        old_w_j = w[j]\n",
    "        w[j] -=  gradient[j]/lips_const[j]\n",
    "        # update gradient:\n",
    "        #gradient = A.T.dot(A.dot(w) - b)\n",
    "        change_w = np.zeros(n_features)\n",
    "        change_w[j] = w[j] - old_w_j\n",
    "        gradient += gram.dot(change_w)\n",
    "        # END TODO\n",
    "        \n",
    "        if t % n_features == 0:\n",
    "            all_objs.append(0.5 * np.linalg.norm(A.dot(w) - b) ** 2)\n",
    "    return w, np.array(all_objs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- compute a precise minimum with your favorite solver\n",
    "- compare the performance of cyclic and greedy CD\n",
    "\n",
    "- could you use greedy CD for unregularized logistic regression? for OLS, but with 100,000 features? Explain your answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Use cyclic_cd method to get a minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "466.3849871861001\n"
     ]
    }
   ],
   "source": [
    "w_cyclic, all_objs_cyclic = cyclic_cd(A, b, n_iter=2000)\n",
    "print(all_objs_cyclic[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.Compare the performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cyclic CD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[53923.83749885 15120.41518928  4174.91851309  1364.94331488\n",
      "   687.23793505   527.33413692   482.5423783    471.27735155\n",
      "   467.8687998    466.82149105   466.51627707   466.42730611\n",
      "   466.39935756   466.3899748    466.38675254   466.38560569\n",
      "   466.38520061   466.38505729   466.38500585   466.38498719]\n"
     ]
    }
   ],
   "source": [
    "print(all_objs_cyclic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "greedy CD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46757.28943372  1658.99242116   637.54200959   499.31713338\n",
      "   473.88588161   468.07350287   466.80716103   466.48707197\n",
      "   466.41380177   466.39263264   466.3869894    466.38556493\n",
      "   466.38515519   466.38503163   466.3849933    466.38498158\n",
      "   466.38497812   466.38497702   466.38497668   466.38497658]\n"
     ]
    }
   ],
   "source": [
    "w_greedy, all_objs_greedy = greedy_cd(A, b, n_iter=2000)\n",
    "print(all_objs_greedy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Greedy CD converges faster than cyclic CD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.could you use greedy CD for unregularized logistic regression? for OLS, but with 100,000 features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use greedy CD for unregularized logistic regression, but we have to midify the lips_const and the gradient in order to suit for the logistic regression. As for OLS with 100,000 features, we can still use greedy CD because it takes only one column of feature in one loop. The number of features won't influence it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Sparse Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An important result\n",
    "\n",
    "Remember: we are solving \n",
    "$$\\hat w \\in \\mathrm{arg \\, min} \\sum_{i=1}^{n} \\mathrm{log} ( 1 + e^{- y_i w^\\top x_i} )  + \\lambda \\Vert w \\Vert_1$$\n",
    "1) Show that:\n",
    "$$ \\lambda \\geq \\lambda_{max} \\implies \\hat w = 0$$\n",
    "where $\\lambda_{max} := \\frac 12 \\Vert X^\\top y \\Vert_{\\infty}$.\n",
    "\n",
    "\n",
    "You will need the following beautiful result: for any $w =(w_1, \\dots, w_p) \\in \\mathbb{R}^p$, the subdifferential of the L1 norm at $w$ is:\n",
    "\n",
    "$$\\partial \\Vert \\cdot \\Vert_1 (w) = \\partial \\vert \\cdot \\vert_1 (w_1)  \\times \\dots \\times \\partial \\vert \\cdot \\vert_1 (w_p) $$\n",
    "where $\\times$ is the Cartesian product between sets,\n",
    "and $$ \\partial \\vert \\cdot \\vert_1 (w_1) = \n",
    "\\begin{cases} &w_j / |w_j| &\\mathrm{if} \\quad w_j \\neq 0, \n",
    "         \\\\ & [-1, 1] &\\mathrm{otherwise.} \n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "(it should now be easy to find $\\partial \\Vert \\cdot \\Vert_1 (\\mathbf{0}_p)$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*answer here*\n",
    "$$\\partial w = \\frac{X^Ty}{n*exp(yXw)}\\ +\\ [-\\lambda,\\lambda]$$\n",
    "if$$ \\lambda \\geq \\lambda_{max},\\ \\ \\  n*exp(yXw)>1$$then we can always find a lambda so that grad_w = 0, since we initialise w at 0,w will stay at 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Show that for sparse Logistic regression the coordinate-wise Lipschitz constant of the smooth term, $\\gamma_j$, can be taken equal to $\\Vert X_j \\Vert^2 / 4$, where $X_j$ denotes the $j$-th column of $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculate the Lipschitz constant by using $\\Vert X \\Vert^2 / 4n$, and for each column of X, Lipschitz constant can be $\\Vert X_j \\Vert^2 / 4$. So $\\gamma_j$ can be taken equal to $\\Vert X_j \\Vert^2 / 4$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are now ready to code cyclic proximal coordinate descent for sparse Logistic regression:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bonus**: show that is possible, when the current iterate is w, to use the better Lipschitz constant \n",
    "    $$L_j = \\sum_{i=1}^n  \\frac{X_{i, j}^2 \\mathrm{e}^{(Xw)_i}}{(1 +\n",
    "\\mathrm{e}^{(Xw)_i})^2}$$\n",
    "    \n",
    "(why is it better?)\n",
    "\n",
    "Implement it in the code with a `better_lc` parameter, and compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y = simu(coefs, n_samples=1000, for_logreg=True)\n",
    "lambda_max = norm(X.T.dot(y), ord= np.inf) / 2.\n",
    "lamb = lambda_max / 10.  \n",
    "# much easier to parametrize lambda as a function of lambda_max than \n",
    "# to take random values like 0.1 in previous Labs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(t):\n",
    "    \"\"\"Sigmoid function\"\"\"\n",
    "    return 1. / (1. + np.exp(-t))\n",
    "\n",
    "\n",
    "def soft_thresh(x, u):\n",
    "    \"\"\"Soft thresholding of x at level u\"\"\"\n",
    "    return np.maximum(0., np.abs(x) - u)\n",
    "\n",
    "\n",
    "def cd_logreg(X, y, lamb, n_iter):\n",
    "    n_samples, n_features = X.shape\n",
    "    w = np.zeros(n_features)\n",
    "    Xw = X.dot(w)\n",
    "    \n",
    "    # TODO\n",
    "    lips_const = norm(X, axis=0) ** 2 / 4.\n",
    "    all_objs = []\n",
    "    # END TODO\n",
    "\n",
    "    for t in range(n_iter):\n",
    "        for j in range(n_features):\n",
    "            old_w_j = w[j]\n",
    "            # TODO\n",
    "            yXw = y * np.dot(X, w)\n",
    "            temp = 1/(1+np.exp(yXw))\n",
    "            grad_j = (-(X.T).dot(y * temp) / n_samples)[j]\n",
    "            w[j] = old_w_j - np.sign(grad_j)*soft_thresh(grad_j, lamb/n_samples)/lips_const[j]\n",
    "            #w[j] = np.sign()*soft_thresh(old_w_j, lips_const[j]*grad_j)\n",
    "            \n",
    "            if old_w_j != w[j]:\n",
    "                #Xw = X.dot(w)\n",
    "                change_w = np.zeros(n_features)\n",
    "                change_w[j] = w[j] - old_w_j\n",
    "                Xw += X.dot(change_w)\n",
    "            #END TODO\n",
    "            \n",
    "        #all_objs[t] = np.log(1. + np.exp(-y * Xw)).sum() + lamb * norm(w, ord=1)\n",
    "        all_objs.append(np.log(1. + np.exp(-y * Xw)).sum() + lamb * norm(w, ord=1))\n",
    "    \n",
    "    return w, np.array(all_objs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "599.0728713408223"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_log, all_objs_log = cd_logreg(X,y,lamb,n_iter=200)\n",
    "all_objs_log[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Real data\n",
    "\n",
    "We will compare vanilla cyclic CD and ISTA to solve the Lasso on a real dataset, called _leukemia_.\n",
    "\n",
    "You can download the file here: http://web.stanford.edu/~hastie/CASI_files/DATA/leukemia_big.csv, and you should place it in the same folder as the current notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(72, 7128)\n",
      "[ 1  1 -1 -1  1  1  1 -1]\n"
     ]
    }
   ],
   "source": [
    "from numpy import genfromtxt\n",
    "y = 2 * (genfromtxt('leukemia_big.csv', delimiter=',', dtype=str)[0] == 'ALL') - 1\n",
    "X = genfromtxt('leukemia_big.csv', delimiter=',')[1:].T\n",
    "\n",
    "print(X.shape)\n",
    "print(y[::10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_max_lasso = norm(X.T.dot(y), ord=np.inf)\n",
    "lambd = lambda_max_lasso / 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code:\n",
    "- a simple proximal gradient solver for the Lasso\n",
    "- a prox CD solver for the Lasso\n",
    "and compare them on this dataset. \n",
    "Do the plots in terms of epochs, not updates (to be fair to CD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a simple proximal gradient for Lasso\n",
    "def prox_lasso(x, s):\n",
    "    \"\"\"Proximal operator for the Lasso at x\"\"\" \n",
    "    return np.sign(x)*np.maximum(np.zeros(x.size),abs(x)-s)\n",
    "    \n",
    "def grad_logreg(w,X,y):   \n",
    "    \"\"\"Logistic gradient\"\"\"\n",
    "    n = X.shape[0]\n",
    "    yXw = y * X.dot(w)\n",
    "    temp = 1/(1+np.exp(yXw))\n",
    "    return -(X.T).dot(y * temp) / n \n",
    "\n",
    "def gd(X, y, lambd, n_iter):\n",
    "    \"\"\"Gradient descent algorithm.\"\"\"\n",
    "    all_objs = []  \n",
    "    n_samples, n_features = X.shape\n",
    "    w = np.zeros(n_features)\n",
    "    L = norm(X, 2) ** 2  / (4. * n_samples)\n",
    "    for i in range(n_iter):\n",
    "        old_w = w\n",
    "        grad = grad_logreg(w,X,y)\n",
    "        w = old_w - np.sign(grad)*soft_thresh(grad,lambd/n_samples)/L\n",
    "        all_objs.append(np.log(1. + np.exp(-y * X.dot(w))).sum() + lambd * norm(w, ord=1))\n",
    "    return w, np.array(all_objs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_log_gd, all_objs_log_gd = gd(X, y , lambd, n_iter=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_log, all_objs_log = cd_logreg(X, y, lambd, n_iter=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_axis = range(20)\n",
    "plt.plot(x_axis, all_objs_log_gd, label = 'Proximal gradient descent')\n",
    "plt.plot(x_axis, all_objs_log, label = 'Proximal coordinate descent')\n",
    "plt.xlabel('epoches')\n",
    "plt.ylabel('all_objs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
